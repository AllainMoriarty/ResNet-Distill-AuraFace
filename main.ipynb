{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfe6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import mxnet as mx\n",
    "from mxnet import recordio\n",
    "import mlflow\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*VerifyOutputSizes.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67bc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomRotation(degrees=10), \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36463b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_path = './VGGFace/train.rec'\n",
    "idx_path = './VGGFace/train.idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb37192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MXNetRecordDataset(Dataset):\n",
    "    def __init__(self, rec_path, idx_path, transform=None):\n",
    "        self.record = recordio.MXIndexedRecordIO(idx_path, rec_path, 'r')\n",
    "        self.keys = list(self.record.keys)\n",
    "        self.transform = transform\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Loop sampai dapat sample valid\n",
    "        for _ in range(len(self.keys)):\n",
    "            key = self.keys[idx]\n",
    "            item = self.record.read_idx(key)\n",
    "\n",
    "            if item is None:\n",
    "                # Ganti index → lanjut ke sample berikut\n",
    "                idx = (idx + 1) % len(self)\n",
    "                continue\n",
    "\n",
    "            header, img_encoded = recordio.unpack(item)\n",
    "            if img_encoded is None or len(img_encoded) == 0:\n",
    "                # Ganti index → lanjut ke sample berikut\n",
    "                idx = (idx + 1) % len(self)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = mx.image.imdecode(img_encoded)\n",
    "            except Exception:\n",
    "                # Kalau decoding gagal, skip\n",
    "                idx = (idx + 1) % len(self)\n",
    "                continue\n",
    "\n",
    "            img = Image.fromarray(img.asnumpy())\n",
    "            label = int(header.label)\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img, label\n",
    "\n",
    "        # Kalau semua gagal (harusnya jarang banget)\n",
    "        raise RuntimeError(\"Semua sample tidak valid dalam .rec file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51185ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformableDataset(Dataset):\n",
    "    def __init__(self, base_dataset, transform=None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base_dataset[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df11fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_datasets(rec_path, idx_path, val_split=0.1, seed=42, max_samples=None, cache_path=None):\n",
    "    full_dataset = MXNetRecordDataset(rec_path, idx_path)  # original dataset\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # coba load cache\n",
    "    if cache_path:\n",
    "        try:\n",
    "            final_indices = np.load(cache_path)\n",
    "            print(f\"Load cached indices from {cache_path}\")\n",
    "        except FileNotFoundError:\n",
    "            final_indices = None\n",
    "    else:\n",
    "        final_indices = None\n",
    "\n",
    "    if max_samples is not None and final_indices is None:\n",
    "        indices_per_label = defaultdict(list)\n",
    "\n",
    "        # buka .rec/.idx menggunakan MXIndexedRecordIO\n",
    "        recordio = mx.recordio.MXIndexedRecordIO(idx_path, rec_path, 'r')\n",
    "        keys = list(recordio.keys)\n",
    "        \n",
    "        for i, key in enumerate(keys):\n",
    "            header, _ = mx.recordio.unpack(recordio.read_idx(key))\n",
    "            label = int(header.label) if isinstance(header.label, float) else int(header.label[0])\n",
    "            indices_per_label[label].append(i)\n",
    "            if (i+1) % 100000 == 0:\n",
    "                print(f\"Processed {i+1}/{len(keys)} records\")\n",
    "\n",
    "        # sampling per label\n",
    "        final_indices = []\n",
    "        for label, indices in indices_per_label.items():\n",
    "            indices = torch.tensor(indices)\n",
    "            if len(indices) > max_samples:\n",
    "                perm = torch.randperm(len(indices), generator=g)[:max_samples]\n",
    "                indices = indices[perm]\n",
    "            final_indices.extend(indices.tolist())\n",
    "\n",
    "        if cache_path:\n",
    "            np.save(cache_path, final_indices)\n",
    "            print(f\"Saved cached indices to {cache_path}\")\n",
    "\n",
    "    if final_indices is not None:\n",
    "        full_dataset = Subset(full_dataset, final_indices)\n",
    "\n",
    "    full_dataset = TransformableDataset(full_dataset)\n",
    "\n",
    "    # split train/val\n",
    "    num_val = int(val_split * len(full_dataset))\n",
    "    num_train = len(full_dataset) - num_val\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset,\n",
    "        [num_train, num_val],\n",
    "        generator=g\n",
    "    )\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3940aa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000/3146439 records\n",
      "Processed 200000/3146439 records\n",
      "Processed 300000/3146439 records\n",
      "Processed 400000/3146439 records\n",
      "Processed 500000/3146439 records\n",
      "Processed 600000/3146439 records\n",
      "Processed 700000/3146439 records\n",
      "Processed 800000/3146439 records\n",
      "Processed 900000/3146439 records\n",
      "Processed 1000000/3146439 records\n",
      "Processed 1100000/3146439 records\n",
      "Processed 1200000/3146439 records\n",
      "Processed 1300000/3146439 records\n",
      "Processed 1400000/3146439 records\n",
      "Processed 1500000/3146439 records\n",
      "Processed 1600000/3146439 records\n",
      "Processed 1700000/3146439 records\n",
      "Processed 1800000/3146439 records\n",
      "Processed 1900000/3146439 records\n",
      "Processed 2000000/3146439 records\n",
      "Processed 2100000/3146439 records\n",
      "Processed 2200000/3146439 records\n",
      "Processed 2300000/3146439 records\n",
      "Processed 2400000/3146439 records\n",
      "Processed 2500000/3146439 records\n",
      "Processed 2600000/3146439 records\n",
      "Processed 2700000/3146439 records\n",
      "Processed 2800000/3146439 records\n",
      "Processed 2900000/3146439 records\n",
      "Processed 3000000/3146439 records\n",
      "Processed 3100000/3146439 records\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = get_train_val_datasets(rec_path, idx_path, val_split=0.2, seed=42, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d229b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.set_transform(train_transform)\n",
    "val_dataset.dataset.set_transform(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097db08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1775623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = ort.SessionOptions()\n",
    "so.log_severity_level = 3\n",
    "teacher_path = './glintr100.onnx'\n",
    "ort_session = ort.InferenceSession(teacher_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'], sess_options=so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641a5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_embedding(images):\n",
    "    resized = F.interpolate(images, size=(112, 112), mode='bilinear', align_corners=False)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: resized.cpu().numpy()}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    embedding = torch.tensor(ort_outs[0]).to(images.device)\n",
    "    return F.normalize(embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a49f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentResNet18(nn.Module):\n",
    "    def __init__(self, embedding_size=512, dropout=0.2):\n",
    "        super(StudentResNet18, self).__init__()\n",
    "        base = models.resnet18(weights='DEFAULT')\n",
    "        base.fc = nn.Identity()\n",
    "        self.backbone = base\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Linear(512, embedding_size)\n",
    "        nn.init.xavier_normal_(self.embedding.weight)\n",
    "        nn.init.constant_(self.embedding.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.bn(features)  \n",
    "        features = self.dropout(features)\n",
    "        embeddings = self.embedding(features)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1, eps=1e-8)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f803b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, temperature=4.0):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.temperature = temperature\n",
    "        self.cosine_loss = nn.CosineEmbeddingLoss(margin=0.0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, student_emb, teacher_emb):\n",
    "        batch_size = student_emb.size(0)\n",
    "        target = torch.ones(batch_size).to(student_emb.device)\\\n",
    "        \n",
    "        cos_loss = self.cosine_loss(student_emb, teacher_emb, target)\n",
    "        mse_loss = self.mse_loss(student_emb / self.temperature, teacher_emb / self.temperature)\n",
    "        \n",
    "        total_loss = self.alpha * cos_loss + self.beta * mse_loss\n",
    "        return total_loss, cos_loss, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6253d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://localhost:9000\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe38ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(student, device, train_loader, val_loader, optimizer, criterion,\n",
    "          epochs=15, resume=False, ckpt_path=\"checkpoint.pth\"):\n",
    "    best_val_cos_sim = 0.0 \n",
    "    patience, patience_counter = 5, 0 \n",
    "    start_epoch = 0\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_cos_sims, val_cos_sims = [], []\n",
    "    train_l2s, val_l2s = [], []\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3,\n",
    "        threshold=0.001, min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    if resume and os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        student.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_cos_sim = checkpoint.get(\"best_val_cos_sim\", 0.0)\n",
    "        print(f\"Resumed training from epoch {start_epoch} (best val cos sim: {best_val_cos_sim:.4f})\")\n",
    "\n",
    "    # Set our tracking server uri for logging\n",
    "    mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\", \"http://127.0.0.1:5000\"))\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ.get(\"MLFLOW_S3_ENDPOINT_URL\", \"http://127.0.0.1:9000\")\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"minioadmin\")\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"minioadmin\")\n",
    "\n",
    "    # Create a new MLflow Experiment\n",
    "    mlflow.set_experiment(\"Face Recognition Knowledge Distillation\")\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"embedding_size\": student.embedding.out_features,\n",
    "            \"dropout\": 0.2,\n",
    "            \"alpha\": criterion.alpha,\n",
    "            \"beta\": criterion.beta,\n",
    "            \"temperature\": criterion.temperature,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"weight_decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "            \"batch_size\": train_loader.batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"patience\": patience,\n",
    "            \"val_split\": 0.2,\n",
    "            \"max_samples\": 25,\n",
    "            \"device\": str(device)\n",
    "        })\n",
    "\n",
    "        # Log model architecture\n",
    "        mlflow.log_param(\"model_architecture\", \"ResNet18\")\n",
    "        mlflow.log_param(\"criterion\", \"CombinedLoss\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        student.train()\n",
    "        train_loss, cos_sim_total, l2_total = 0, 0, 0\n",
    "        train_cos_loss_total, train_mse_loss_total = 0, 0\n",
    "\n",
    "        for images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                stud_emb = student(images)\n",
    "                with torch.no_grad():\n",
    "                    teach_emb = get_teacher_embedding(images).to(device)\n",
    "\n",
    "                total_loss, cos_loss, mse_loss = criterion(stud_emb, teach_emb)\n",
    "\n",
    "            # Backward pass dengan gradient scaling\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            # Gradient clipping untuk stabilitas\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Metrics tracking\n",
    "            train_loss += total_loss.item()\n",
    "            train_cos_loss_total += cos_loss.item()\n",
    "            train_mse_loss_total += mse_loss.item()\n",
    "            cos_sim_total += F.cosine_similarity(stud_emb, teach_emb, dim=1).mean().item()\n",
    "            l2_total += torch.norm(stud_emb - teach_emb, dim=1).mean().item()\n",
    "\n",
    "        # Average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_cos = cos_sim_total / len(train_loader)\n",
    "        avg_train_l2 = l2_total / len(train_loader)\n",
    "        avg_train_cos_loss = train_cos_loss_total / len(train_loader)\n",
    "        avg_train_mse_loss = train_mse_loss_total / len(train_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_cos_sims.append(avg_train_cos)\n",
    "        train_l2s.append(avg_train_l2)\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss, cos_sim_total, l2_total = 0, 0, 0\n",
    "        val_cos_loss_total, val_mse_loss_total = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                images = images.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    stud_emb = student(images)\n",
    "                    teach_emb = get_teacher_embedding(images).to(device)\n",
    "                    total_loss, cos_loss, mse_loss = criterion(stud_emb, teach_emb)\n",
    "\n",
    "                val_loss += total_loss.item()\n",
    "                val_cos_loss_total += cos_loss.item()\n",
    "                val_mse_loss_total += mse_loss.item()\n",
    "                cos_sim_total += F.cosine_similarity(stud_emb, teach_emb, dim=1).mean().item()\n",
    "                l2_total += torch.norm(stud_emb - teach_emb, dim=1).mean().item()\n",
    "\n",
    "        # Average validation metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_cos = cos_sim_total / len(val_loader)\n",
    "        avg_val_l2 = l2_total / len(val_loader)\n",
    "        avg_val_cos_loss = val_cos_loss_total / len(val_loader)\n",
    "        avg_val_mse_loss = val_mse_loss_total / len(val_loader)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_cos_sims.append(avg_val_cos)\n",
    "        val_l2s.append(avg_val_l2)\n",
    "\n",
    "        # Log the loss metric\n",
    "        metrics = {\n",
    "            f\"train_loss_epoch_{epoch+1}\": avg_train_loss,\n",
    "            f\"train_cos_sim_epoch_{epoch+1}\": avg_train_cos,\n",
    "            f\"train_l2_epoch_{epoch+1}\": avg_train_l2,\n",
    "            f\"train_cos_loss_epoch_{epoch+1}\": avg_train_cos_loss,\n",
    "            f\"train_mse_loss_epoch_{epoch+1}\": avg_train_mse_loss,\n",
    "            f\"val_loss_epoch_{epoch+1}\": avg_val_loss,\n",
    "            f\"val_cos_sim_epoch_{epoch+1}\": avg_val_cos,\n",
    "            f\"val_l2_epoch_{epoch+1}\": avg_val_l2,\n",
    "            f\"val_cos_loss_epoch_{epoch+1}\": avg_val_cos_loss,\n",
    "            f\"val_mse_loss_epoch_{epoch+1}\": avg_val_mse_loss,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        mlflow.log_metrics(metrics, step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train: Loss={avg_train_loss:.4f}, CosSim={avg_train_cos:.4f}, L2={avg_train_l2:.4f}, \"\n",
    "              f\"CosLoss={avg_train_cos_loss:.4f}, MSELoss={avg_train_mse_loss:.4f}\")\n",
    "        print(f\"       | \"\n",
    "              f\"Val: Loss={avg_val_loss:.4f}, CosSim={avg_val_cos:.4f}, L2={avg_val_l2:.4f}, \"\n",
    "              f\"CosLoss={avg_val_cos_loss:.4f}, MSELoss={avg_val_mse_loss:.4f}\")\n",
    "\n",
    "        # Learning rate scheduling berdasarkan validation cosine similarity\n",
    "        scheduler.step(avg_val_cos)\n",
    "\n",
    "        # Early stopping berdasarkan cosine similarity\n",
    "        if avg_val_cos > best_val_cos_sim:\n",
    "            best_val_cos_sim = avg_val_cos\n",
    "            torch.save(student.state_dict(), 'best_student.pth')\n",
    "            print(f\"*** New best model saved! Cosine Similarity: {best_val_cos_sim:.4f} ***\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation cosine similarity: {best_val_cos_sim:.4f}\")\n",
    "                break\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": student.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_val_cos_sim\": best_val_cos_sim,\n",
    "        }, ckpt_path)\n",
    "\n",
    "        # Log final metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"final_best_val_cos_sim\": best_val_cos_sim,\n",
    "            \"final_train_loss\": avg_train_loss,\n",
    "            \"final_val_loss\": avg_val_loss\n",
    "        })\n",
    "\n",
    "        # Log model artifacts\n",
    "        mlflow.pytorch.log_model(student, \"final_model\")\n",
    "        mlflow.log_artifact(ckpt_path)\n",
    "\n",
    "    return train_losses, val_losses, train_cos_sims, val_cos_sims, train_l2s, val_l2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933da034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Active provider:\", ort_session.get_providers())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "142a3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39103/3006762444.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run delightful-foal-3 at: http://localhost:5000/#/experiments/2/runs/a603f568b05e4bc9b48f3c52502327d2\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f503e000ad4d6b9534c9b0c3644961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15 - Training:   0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39103/3006762444.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d620782f257041f0aed3d05b97dc2f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15 - Validation:   0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39103/3006762444.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: Loss=0.4269, CosSim=0.3901, L2=1.0688, CosLoss=0.6099, MSELoss=0.0001\n",
      "       | Val: Loss=0.3717, CosSim=0.4691, L2=0.9804, CosLoss=0.5309, MSELoss=0.0001\n",
      "*** New best model saved! Cosine Similarity: 0.4691 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 10:58:04 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/09/26 10:58:06 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu129) contains a local version label (+cu129). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/26 10:58:11 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.23.0+cu129) contains a local version label (+cu129). MLflow logged a pip requirement for this package as 'torchvision==0.23.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/26 10:58:12 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/09/26 10:58:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f9cdc310ab46509f7159a9d2df5f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15 - Training:   0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39103/3006762444.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m criterion = CombinedLoss(alpha=\u001b[32m0.7\u001b[39m, beta=\u001b[32m0.3\u001b[39m, temperature=\u001b[32m4.0\u001b[39m)\n\u001b[32m      4\u001b[39m optimizer = optim.AdamW(student.parameters(), lr=\u001b[32m5e-5\u001b[39m, weight_decay=\u001b[32m1e-4\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.999\u001b[39m), eps=\u001b[32m1e-8\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(student, device, train_loader, val_loader, optimizer, criterion, epochs, resume, ckpt_path)\u001b[39m\n\u001b[32m     71\u001b[39m     stud_emb = student(images)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m         teach_emb = \u001b[43mget_teacher_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     75\u001b[39m     total_loss, cos_loss, mse_loss = criterion(stud_emb, teach_emb)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Backward pass dengan gradient scaling\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_teacher_embedding\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m      2\u001b[39m resized = F.interpolate(images, size=(\u001b[32m112\u001b[39m, \u001b[32m112\u001b[39m), mode=\u001b[33m'\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m'\u001b[39m, align_corners=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m ort_inputs = {ort_session.get_inputs()[\u001b[32m0\u001b[39m].name: resized.cpu().numpy()}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ort_outs = \u001b[43mort_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m embedding = torch.tensor(ort_outs[\u001b[32m0\u001b[39m]).to(images.device)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.normalize(embedding, p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/allain/Workspace/Intern/resnet34-distill/.venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    271\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "student = StudentResNet18(embedding_size=512, dropout=0.2).to(device)\n",
    "criterion = CombinedLoss(alpha=0.7, beta=0.3, temperature=4.0)\n",
    "optimizer = optim.AdamW(student.parameters(), lr=5e-5, weight_decay=1e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "results = train(student, device, train_loader, val_loader, optimizer, criterion, epochs=15, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "student = StudentResNet18(embedding_size=512, dropout=0.3).to(device)\n",
    "criterion_cos = nn.CosineEmbeddingLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "ckpt_path = './checkpoint.pth'\n",
    "\n",
    "results = train(student, device, train_loader, val_loader, optimizer, criterion_cos, criterion_mse, epochs=15, resume=True, ckpt_path=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
